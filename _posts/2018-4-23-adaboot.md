---
layout: post

title: "Giải phẫu thuật toán Adaboost!"
description: "tìm hiểu thuật toán adaboost"
categories: [machine_learning]
tags: [machine_learning]
redirect_from:
  - /2018/04/23/
---
## Mở đầu
* Mục lục bài viết .
  * Tìm hiểu thuật toán
  * Giải phẫu thuật toán
  * Minh họa hình vẽ
  * Ví dụ minh họa
## Tìm hiểu thuật toán
* Thuật toán adaboost là một trong những thuật toán ensemble learning , thuộc loại bagging cùng với Gradient boosting. Ý tưởng của thuật toán này là kết hợp các thuật toán yếu thành một thuật toán mạnh,ban đầu các instance được gắn trọng số như sau nhưng sau khi classifier bằng một thuật toán đơn giản như (các thuật toán tree chẳng hạn) những instance nào mà classifier sai thì được gắn trọng số cao hơn và ngược lại, cái ý tưởng ví như quay đầu là bờ trong phật giáo . Nhằm giúp các instance gắn sai label có nhiều cơ hội classifer đúng label hơn. Ta có giải thuật chung cho bài toán này như sau :
  * Ban đầu ta gắn tỉ trọng cho các data theo phân phối chuẩn tức là $ \alpha_{i} = \frac{1}{N}$
  * For t = 1,..T:
    * training $ f_{t}(x)$ với tỉ trọng $ \alpha_{i} $
    * Tính toán lại hệ số $ W_{t} $
    * Tính toán lại tỉ trọng $ \alpha_{i} $
  * Model predict cuối cùng sẽ là : $ y = (\sum_(t=1}^{T}.W_{t}.f_{t}(x))$
* Trong đó $ \alpha_{i}$ là tỉ trọng mỗi quan sát, $ f_{t}(x)$  là thuật toán training t, $ W_{t} $ là hệ số của $ f_{t}(x)$. Ta cùng tìm hiểu chi tiết thuật toán qua phần tiếp theo.
## Giải phẫu thuật toán
* Ta sẽ tìm hiểu các công thức trước khi xâu chuỗi nó vào thuật toán.
* Thứ nhất cách tính $ W_{t} $:
  * Đầu tiên ta sẽ tính toán tỉ trọng weight error tức là tỉ trọng classifier sai trên tổng classifier.
    * tổng weight of mistake : $ \sum_{i=1}^{N}(\alpha_{i}(y_{true} \neq y_{pre}) $
    * Tổng weight của data : $\sum_{i=1}^{N}(\alpha_{i})$
    * weight error chỉnh là tỉ số weight of mistake với weight all data : $ \frac{\text{weight of mistake}}{weight all data} $
   * $ W_{t} $ của  $f_{t}(x)$ được tính theo công thức :$ W_{t} = \frac{1}{2}ln\frac{1 - weight_error(f_{t})}{weight_error(f_{t}} $
   * Chúng ta thấy rằng nếu ` weight error` thấp thì $f_{t}(x)$ là thuật toán training good nên $W_{t}$ tương ứng sẽ cao. Tức là tỉ trọng sẽ cao khi thuật toán training google và ngược lại. Nếu random tức 50-50 thì  $W_{t}$ sẽ là 0.
* Thứ 2, cập nhật lại trọng số cho data. Những instance nào mà classifier true thì sẽ giảm tỉ trọng xuống, còn wrong thì sẽ tăng tỉ trọng lên để nó có nhiều cơ hội classifier true trong lần training tiếp theo hơn.
$$
\alpha_{i+1} =  \begin{cases}
  \alpha_{i}e^{-w_{i}}, & if f_{i}(x_{i}} = y_{i} \\
 \alpha_{i}e^{w_{i}}, & if f_{i}(x_{i}} \neq y_{i}}
 \end{cases}
$$
* Có một vấn đề xảy ra là tỉ trọng $\alpha_{i}$ sẽ thay đổi theo $e^{-+w_{i}$ tức là dạng lũy thừa. Khí đó true class sẽ có tỉ trọng rất nhỏ và wrong class sẽ có tỉ trọng rất lớn. Vì thế người ta sẽ normalizer $\alpha_{i}$  sau mỗi lần training t để khắc phục vấn đề đó.
$$
$\alpha_{i normalize}$ = \frac{\alpha_{i}}{\sum_{i=1}^{N}(\alpha_{i}} 
$$
## Minh họa hình vẽ
## Ví dụ minh họa .
