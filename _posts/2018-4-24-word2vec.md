---
layout: post
title: "Tìm hiểu về Word2Vec"
description: "Tìm hiểu về word2vec"
categories: [deep_learning]
tags: [python,machine learning]
redirect_from:
  - /2018/04/21/
---
Mở đầu
Như chúng ta đã biết máy tính được cấu tạo từ những con số, do đó nó chỉ có thể đọc được dữ liệu số mà thôi. Trong natural language processing
thì để xử lý dữ liệu text chúng ta cũng phải chuyển dữ liệu từ text sang numeric, tức là đưa nó vào một không gian mới người ta thường
gọi là embbding. Trước đây người ta mã hóa theo kiểu one hot encoding tức là tạo  một vocabualary cho dữ liệu và mã hóa các word trong document
thành những vectoc, nếu word đó có trong document thì mã hóa là 1 còn không có sẽ là 0. Kết quả tạo ra một sparse matrix, tức là matrix hầu hết 
là 0.Các mã hóa này có nhiều nhược điểm đó là thứ nhất là số chiều của nó rất lớn (NxM, N là số document còn M là số vocabulary), thứ 2 các word
không có quan hệ với nhau. Điều đó dẫn đến người ta nghĩ ra một model mới có tên là word2Vec, ở đó các word sẽ có quan hệ với nhau về semantic
tức là ví dụ như paris-tokyo,man-women,boy-girl những cặp từ này sẽ có khoảng cách gần nhau hơn trong word2vec space. Ví dụ điển hình mà ta thây
đó là phương trình king - queen = man - women . Cái ưu điểm thứ 2 là số chiều của nó sẽ giảm chỉ còn NxD( trong đó D là số chiều của Word2vec).
Word2vec có rất nhiều model khác nhau nhưng điển hình thì có skip-gram,Cbow va Glove.
  *
