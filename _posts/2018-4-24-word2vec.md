---
layout: post
title: "Tìm hiểu về Word2Vec"
description: "Tìm hiểu về word2vec"
categories: [deep_learning]
tags: [python,machine learning]
redirect_from:
  - /2018/04/21/
---
Mở đầu
Như chúng ta đã biết máy tính được cấu tạo từ những con số, do đó nó chỉ có thể đọc được dữ liệu số mà thôi. Trong natural language processing
thì để xử lý dữ liệu text chúng ta cũng phải chuyển dữ liệu từ text sang numeric, tức là đưa nó vào một không gian mới người ta thường
gọi là embbding. Trước đây người ta mã hóa theo kiểu one hot encoding tức là tạo  một vocabualary cho dữ liệu và mã hóa các word trong document
thành những vectoc, nếu word đó có trong document thì mã hóa là 1 còn không có sẽ là 0. Kết quả tạo ra một sparse matrix, tức là matrix hầu hết 
là 0.Các mã hóa này có nhiều nhược điểm đó là thứ nhất là số chiều của nó rất lớn (NxM, N là số document còn M là số vocabulary), thứ 2 các word
không có quan hệ với nhau. Điều đó dẫn đến người ta nghĩ ra một model mới có tên là **Word embbding**, ở đó các word sẽ có quan hệ với nhau về semantic
tức là ví dụ như paris-tokyo,man-women,boy-girl những cặp từ này sẽ có khoảng cách gần nhau hơn trong Word embbding space. Ví dụ điển hình mà ta thây
đó là phương trình king - queen = man - women . Cái ưu điểm thứ 2 là số chiều của nó sẽ giảm chỉ còn NxD.
Word embbding có 2 model nổi tiếng là word2vec và Glove.
  * Word2vec được tạo ra năm 2013 bởi một kỹ sư ở google có tên là **Tomas Mikolov**. Nó là một model unsupervised learning,được training từ large corpus . Chiều của Word2vec nhỏ hơn nhiều so với one-hot-encoding, với số chiều là NxD với N là Number of document và D là số chiều word embedding . Word2vec có 2 model là skip-gram và Cbow.
      * Skip-gram model là model predict word surrounding khi cho một từ cho trước, ví dụ như text = "I love you so much". Khi dùng 1 window search có size 3 ta thu được  : {(i,you),love},{(love,so),you},{(you,much),so}. Nhiệm vụ của nó là khi cho 1 từ center ví dụ là love thì phải predict các từ xung quang là i, you.
      * Cbow là viết tắt của continous bag of word . Model này ngược với model skip-gram tức là cho những từ surrounding predict word current.
      * Trong thực tế người ta chỉ chọn một trong  2 model để training, Cbow thì training nhanh hơn nhưng độ chính xác không cao bằng skip-gram và ngược lại
 * Glove cũng được tạo ra năm 2013 bởi một nhóm nghiên cứu ở stanford. Nó dựa trên word-count base model. Nó dùng kỹ thuật matrix factorization để đưa matrix ban đầu về các matrix nhỏ hơn tương tự như model ở recommend system. Mình chưa nghiên cứu model này có gì nó nói lại sau qua các bài viết khác.
 
Cấu trúc bài:
  * Math of Word2vec 
  * Build model from scratch.
  * Buil model from library
## Math of Word2vec
* Trong bài này ta chỉ tìm hiểu model Skip-gram model. Cbow là model ngược lại. Skip-gram model có cấu trúc như hình vẽ dưới đây.

!["skip_gram](/assets/images/word2vec1.jpg)

* Input là one-hot-vector mỗi word sẽ có dạng ${x_{1},x_{2},..x_{v}}$ trong đó V là số vocabulary, là một vector trong đó mỗi word sẽ có
giá trị 1 tương đương với index trong vocabulary và còn lại sẽ là 0.
* Weight matrix giữa input và hidden layer là matrix W(có dimention VxN) có active function là linear, weight giữa hidden và out put là $ W^{'}$ (có dimention là NxV) active function của out put là soft max.
* Mỗi row của W là vector N chiều đại diện cho $ v_{w} $ là mỗi word trong input layer.Mỗi row của W là $ v_{w}^{T}$ . Lưu ý là input là 1 one hot vector (sẽ có dạng 000100) chỉ có 1 phần tử bằng 1 nên.

$$
h = W_{T}x = v_{w}^{T}
$$

* Từ hidden layer đến out put là matrix $ W^{'} = {w_{i,j}^{'}} $ . Ta tính score $u_{i} $ cho mỗi word trong vocabulary.

$$
u_{i} = v_{w_{j}}^{'}h
$$ 
* Trong đó $v_{w_{j}}$ là vector colum j trong $ W^{'} $. Tiếp đó ta sử dụng soft max funtion.

$$ 
P(w_{j}|w_{I}) = y_{i} = \frac{exp(u_{j})}{\sum_{j^{'}=1}^{V}exp(u_{j^{'}})} 
$$
